# 机器学习算法之Logistic回归

## 1.Logistic回归

![1527561478983](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821717.png)

假设现在有一些数据点，我们利用一条直线对这些点进行拟合(该线称为最佳拟合直线)，这个拟合过程就称作为回归。

首先学习下sigmoid函数，也可称其为Logistic函数

![1527561570951](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821723.png)

![1527561596571](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821726.png)

![](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821738.png)

整合后，得sigmoid函数：

![](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821740.jpg)

sigmoid函数图像如下所示:

![1527561695199](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821743.png)

z是一个矩阵，θ是参数列向量(要求解的)，x是样本列向量(给定的数据集)。θ^T表示θ的转置。**g(z)函数实现了任意实数到[0,1]的映射，这样我们的数据集([x0,x1,…,xn])，不管是大于1或者小于0，都可以映射到[0,1]区间进行分类。**hθ(x)给出了输出为1的概率。比如当hθ(x)=0.7，那么说明有70%的概率输出为1。输出为0的概率是输出为1的补集，也就是30%。

如果我们有合适的参数列向量θ([θ0,θ1,…θn]^T)，以及样本列向量x([x0,x1,…,xn])，那么我们对样本x分类就可以通过上述公式计算出一个概率，如果这个概率大于0.5，我们就可以说样本是正样本，否则样本是负样本。

举个例子，对于”垃圾邮件判别问题”，对于给定的邮件(样本)，我们定义非垃圾邮件为正类，垃圾邮件为负类。我们通过计算出的概率值即可判定邮件是否是垃圾邮件。

**接下来寻找合适的参数向量θ**

由sigmoid的函数特性，做出如下解释:

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821741.jpg)

上式即为在已知样本x和参数θ的情况下，样本x属性正样本(y=1)和负样本(y=0)的条件概率。理想状态下，根据上述公式，求出各个点的概率均为1，也就是完全分类都正确。但是考虑到实际情况，样本点的概率越接近于1，其分类效果越好。比如一个样本属于正样本的概率为0.51，那么我们就可以说明这个样本属于正样本。另一个样本属于正样本的概率为0.99，那么我们也可以说明这个样本属于正样本。但是显然，第二个样本概率更高，更具说服力。我们可以把上述两个概率公式合二为一：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821743.png)

合并出来的Cost，我们称之为代价函数(Cost Function)。当y等于1时，(1-y)项(第二项)为0；当y等于0时，y项(第一项)为0。为了简化问题，我们对整个表达式求对数，(将指数问题对数化是处理数学问题常见的方法)：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821744.png)

这个代价函数，是对于一个样本而言的。给定一个样本，我们就可以通过这个代价函数求出，样本所属类别的概率，而这个概率越大越好，所以也就是求解这个代价函数的最大值。既然概率出来了，那么最大似然估计也该出场了。假定样本与样本之间相互独立，那么整个样本集生成的概率即为所有样本生成概率的乘积，再将公式对数化，便可得到如下公式：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821744.png)

梯度上升公式如下:

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821745.png)

若要求J(θ)的极大值，可以对J(θ)求推导:

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821745.png)

其中

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821745.png)

再由：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821746.png)

可得：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821746.png)

第三部分为:

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821746.png)

综上可得：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821747.png)

最后可得梯度上升迭代公式为：

![img](http://pcv9xat5l.bkt.clouddn.com/机器学习算法之Logistic回归/201883/1533282821747.png)